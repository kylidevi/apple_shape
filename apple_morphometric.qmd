---
title: "apple morphometric analysis"
format: 
  html:
    fig-format: pdf 
editor: visual
---

<!-- Preliminary modules, libraries and functions for the rest of the code to function -->

```{r}
#| label: R Libraries
#| echo: false
#########################
### LOAD IN LIBRARIES ###
#########################

# python libraries that will be used in the python sections
library(reticulate)
# py_install("opencv-python")
# py_install("scikit-learn")
# py_install("matplotlib")
# py_install("pandas")
# py_install("numpy")
# py_install("seaborn")
```

```{python}
#| label: Python Libraries
#| echo: false
#######################
### LOAD IN MODULES ###
#######################

import cv2 # to install on mac: pip install opencv-python
from scipy.interpolate import interp1d # for interpolating points
from sklearn.decomposition import PCA # for principal component analysis
from scipy.spatial import procrustes # for Procrustes analysis
from scipy.spatial import ConvexHull # for convex hull
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis # for LDA
from sklearn.metrics import confusion_matrix # for confusion matrix
from os import listdir # for retrieving files from directory
from os.path import isfile, join # for retrieving files from directory
import matplotlib.pyplot as plt # for plotting
import numpy as np # for using arrays
import math # for mathematical operations
import pandas as pd # for using pandas dataframes
import seaborn as sns # for plotting in seaborn
from matplotlib.colors import LogNorm
```

```{python}
#| label: Python Functions
#| echo: false
#################
### FUNCTIONS ###
#################

def angle_between(p1, p2, p3):
    """
    define a function to find the angle between 3 points anti-clockwise in degrees, p2 being the vertex
    inputs: three angle points, as tuples
    output: angle in degrees
    """
    x1, y1 = p1
    x2, y2 = p2
    x3, y3 = p3
    deg1 = (360 + math.degrees(math.atan2(x1 - x2, y1 - y2))) % 360
    deg2 = (360 + math.degrees(math.atan2(x3 - x2, y3 - y2))) % 360
    return deg2 - deg1 if deg1 <= deg2 else 360 - (deg1 - deg2)

def rotate_points(xvals, yvals, degrees):
    """"
    define a function to rotate 2D x and y coordinate points around the origin
    inputs: x and y vals (can take pandas dataframe columns) and the degrees (positive, anticlockwise) to rotate
    outputs: rotated and y vals
    """
    angle_to_move = 90-degrees
    rads = np.deg2rad(angle_to_move)
    
    new_xvals = xvals*np.cos(rads)-yvals*np.sin(rads)
    new_yvals = xvals*np.sin(rads)+yvals*np.cos(rads)
    
    return new_xvals, new_yvals

def interpolation(x, y, number): 
    """
    define a function to return equally spaced, interpolated points for a given polyline
    inputs: arrays of x and y values for a polyline, number of points to interpolate
    ouputs: interpolated points along the polyline, inclusive of start and end points
    """
    distance = np.cumsum(np.sqrt( np.ediff1d(x, to_begin=0)**2 + np.ediff1d(y, to_begin=0)**2 ))
    distance = distance/distance[-1]

    fx, fy = interp1d( distance, x ), interp1d( distance, y )

    alpha = np.linspace(0, 1, number)
    x_regular, y_regular = fx(alpha), fy(alpha)
    
    return x_regular, y_regular

def euclid_dist(x1, y1, x2, y2):
    """
    define a function to return the euclidean distance between two points
    inputs: x and y values of the two points
    output: the eulidean distance
    """
    return np.sqrt((x2-x1)**2 + (y2-y1)**2)

def poly_area(x,y):
    """
    define a function to calculate the area of a polygon using the shoelace algorithm
    inputs: separate numpy arrays of x and y coordinate values
    outputs: the area of the polygon
    """
    return 0.5*np.abs(np.dot(x,np.roll(y,1))-np.dot(y,np.roll(x,1)))

def gpa_mean(leaf_arr, landmark_num, dim_num):
    
    """
    define a function that given an array of landmark data returns the Generalized Procrustes Analysis mean
    inputs: a 3 dimensional array of samples by landmarks by coordinate values, number of landmarks, number of dimensions
    output: an array of the Generalized Procrustes Analysis mean shape
    
    """

    ref_ind = 0 # select arbitrary reference index to calculate procrustes distances to
    ref_shape = leaf_arr[ref_ind, :, :] # select the reference shape

    mean_diff = 10**(-30) # set a distance between means to stop the algorithm

    old_mean = ref_shape # for the first comparison between means, set old_mean to an arbitrary reference shape

    d = 1000000 # set d initially arbitraily high

    while d > mean_diff: # set boolean criterion for Procrustes distance between mean to stop calculations

        arr = np.zeros( ((len(leaf_arr)),landmark_num,dim_num) ) # empty 3D array: # samples, landmarks, coord vals

        for i in range(len(leaf_arr)): # for each leaf shape 

            s1, s2, distance = procrustes(old_mean, leaf_arr[i]) # calculate procrustes adjusted shape to ref for current leaf
            arr[i] = s2 # store procrustes adjusted shape to array

        new_mean = np.mean(arr, axis=(0)) # calculate mean of all shapes adjusted to reference

        s1, s2, d = procrustes(old_mean, new_mean) # calculate procrustes distance of new mean to old mean

        old_mean = new_mean # set the old_mean to the new_mea before beginning another iteration

    return new_mean
```

<!-- Following sections are for reading in the data and does the preliminary image analysis -->

### Apple Data

#### Metadata Listing

```{python}
#| label: Apple Metadata
########################
### READ IN METADATA ###
########################

# read in csv from directory
mdata = pd.read_csv("C:/Users/User/Desktop/COOP02/code/apple_shape/output_final/apple_metadata.csv") 

mdata.head() # head data to check
```

```{python}
#| label: Apple List
#######################################
### MAKE A LIST OF IMAGE FILE NAMES ###
#######################################

# set image directory
data_dir = "C:/Users/User/Desktop/COOP02/code/apple_shape/binary_domestica/" 

file_names = [f for f in listdir(data_dir) if isfile(join(data_dir, f))] # create a list of file names

#file_names.remove('.DS_Store') # remove .DS_Store file

file_names.sort() # sort the list of file names

file_names[slice(50)] # check list of file names, only first 50
```

#### Process and Landmarking

-   Read in image in grayscale
-   Select the contour of the largest object (the leaf)
-   Interpolate with a high resolution of pseudo-landmarks
-   Find the bottom and top index point on the high resolution contour
-   Reset the bottom index to zero
-   Interpolate each side with desired number of equidistant pseudo-landmarks
-   Rotate leaves and scale to centimeters
-   Save pseudo-landmarks scaled to centimeters in an array

##### PARAMETERS AND INDEXING:

-   `high_res_pts` is an arbitrarily high number of points to initially interpolate
-   `res` is the desired number of points to interpolate on each side of the leaf
-   The total number of pseudo-landmarks will be `2*res - 1`
-   The bottom index will be `0`
-   The top index will be `res-1`
-   The returned apples in `apple_cm_arr` are scaled in size to centimeters

```{python}
#| label: Apple Parameters
######################
### SET PARAMETERS ###
######################

# the number of equidistant points to create
# an initial high resolution outline of the leaf
high_res_pts = 1000 

# the ultimate number of equidistant points on each side of the apple
# (-1 for the top)
# the apple will have res*2-1 pseudo-landmarks
#################
#################
#################
res = 50 ########
#################
#################
#################

# an array to store pseudo-landmarks
apple_cm_arr = np.zeros((len(mdata),(res*2)-1,2))

# for each apple . . .
for lf in range(len(mdata)):

    ###############################
    ### READ IN GRAYSCALE IMAGE ###
    ###############################

    curr_image = mdata["file"][lf] # select the current image
    print(lf, curr_image) # print each leaf in case there are problems later
    
    # read in image
    # convert to grayscale
    # invert the binary
    img = cv2.bitwise_not(cv2.cvtColor(cv2.imread(data_dir + curr_image),cv2.COLOR_BGR2GRAY))

    # find contours of binary objects
    contours, hierarchy = cv2.findContours(img,  
        cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    ##############################
    ### SELECT LARGEST CONTOUR ###
    ##############################

    # ideally there is only one apple in the image
    # in the case there are smaller objects
    # this code selects the largest object (the apple)
    # if there is one and only one object in the image
    # then the following code is not necessary

    x_conts = [] # list of lists of contour x vals
    y_conts = [] # list of lists of contour y vals
    areas_conts = [] # list of bounding box areas of contours
    for c in contours: # for each contour
        x_vals = [] # store x vals for current contour 
        y_vals = [] # store y vals for current contour
        for i in range(len(c)): # for each point in current contour
            x_vals.append(c[i][0][0]) # isolate x val
            y_vals.append(c[i][0][1]) # isolate y val
        area = (max(x_vals) - min(x_vals))*(max(y_vals) - min(y_vals)) # calculate bounding box area of contour
        x_conts.append(x_vals) # append the current contour x vals
        y_conts.append(y_vals) # append the current contour y vals
        areas_conts.append(area) # append the current contour bounding box areas

    area_inds = np.flip(np.argsort(areas_conts)) # get indices to sort contours by area
    sorted_x_conts = np.array(x_conts, dtype=object)[area_inds][0:] # areas sorted largest to smallest, x vals
    sorted_y_conts = np.array(y_conts, dtype=object)[area_inds][0:] # areas sorted largest to smallest, y vals

    ################################################
    ### INTERPOLATE HIGH RES NUMBER OF LANDMARKS ###
    ################################################

    # convert the apple to high resolution number of landmarks
    # using high_res_pt value
    # need to convert arrays of pixel int to floats first
    high_res_x, high_res_y = interpolation(np.array(sorted_x_conts[0], dtype=np.float32), 
                                           np.array(sorted_y_conts[0], dtype=np.float32), high_res_pts)

    #################################
    ### FIND BOTTOM AND TOP INDEX ###
    #################################

    # get the bottom and top landmark point values
    bottom_pt = np.array((mdata["bottom_x"][lf], mdata["bottom_y"][lf]))
    top_pt = np.array((mdata["top_x"][lf], mdata["top_y"][lf]))

    bottom_dists = [] # store distance of each high res point to bottom
    top_dists = [] # store distance of each high res point to top

    for pt in range(len(high_res_x)): # for each of the high resolution points

        # euclidean distance of the current point from the bottom and top landmark
        ed_bottom = euclid_dist(bottom_pt[0], bottom_pt[1], high_res_x[pt], high_res_y[pt])
        ed_top = euclid_dist(top_pt[0], top_pt[1], high_res_x[pt], high_res_y[pt])

        # store distance of current point from bottom/top
        bottom_dists.append(ed_bottom)
        top_dists.append(ed_top)

    # get index of bottom and top points
    bottom_ind = np.argmin(bottom_dists)
    top_ind = np.argmin(top_dists)

    ##################################
    ### RESET BOTTOM INDEX TO ZERO ###
    ##################################

    # reset bottom index position to zero
    high_res_x = np.concatenate((high_res_x[bottom_ind:],high_res_x[:bottom_ind]))
    high_res_y = np.concatenate((high_res_y[bottom_ind:],high_res_y[:bottom_ind]))

    # recalculate indices with new indexing
    top_ind = top_ind-bottom_ind # note: negative index if top_ind<bottom_ind
    bottom_ind = bottom_ind-bottom_ind

    # create single array for apple coordinates
    lf_contour = np.column_stack((high_res_x, high_res_y))

    ##############################################################
    ### INTERPOLATE EACH SIDE WITH DESIRED NUMBER OF LANDMARKS ###
    ##############################################################

    # interpolate at desired resolution the left and right sides of the apple
    left_inter_x, left_inter_y = interpolation(lf_contour[bottom_ind:top_ind+1,0],lf_contour[bottom_ind:top_ind+1,1],res)
    right_inter_x, right_inter_y = interpolation(lf_contour[top_ind:,0],lf_contour[top_ind:,1],res)

    # the start of the right side and end of the left side
    # both contain the top landmark
    # delete the last point on the left side
    left_inter_x = np.delete(left_inter_x, -1)
    left_inter_y = np.delete(left_inter_y, -1)

    # BOTTOM OF APPLE IS INDEX 0
    # TOP INDEX IS RES-1 IF BOTH LEFT & RIGHT POINTS
    # TOTAL PSEUDOLANDMARKS IS 2*RES-1
    lf_pts_left = np.column_stack((left_inter_x, left_inter_y))
    lf_pts_right = np.column_stack((right_inter_x, right_inter_y))
    lf_pts = np.row_stack((lf_pts_left, lf_pts_right))

    ##########################################################
    ### ROTATE APPLES UPWARD AND SCALE SIZE TO CENTIMETERS ###
    ##########################################################

    top_point = lf_pts[res-1,:] # get top point
    bottom_point = lf_pts[0,:] # get bottom point

    # calculate angle between top. bottom, and an arbitrary reference
    ang = angle_between(top_point, bottom_point, (bottom_point[0]+1,bottom_point[1]) )

    # rotate points upwards
    rot_x, rot_y = rotate_points(lf_pts[:,0], lf_pts[:,1], ang) 
    rot_pts = np.column_stack((rot_x, rot_y))
    
    # calculate apple area in pixels^2
    lf_area_px2 = poly_area(rot_pts[:,0], rot_pts[:,1])

    # scale apple into cm
    cm_lf = rot_pts/(81.93)
    
    # store the apple scaled into cm into the apple_cm_arr
    apple_cm_arr[lf,:,:] = cm_lf
```

### Pseudo-landmarking

```{python}
#| label: Pseudo-landmarks
# Plot each apple

plt.figure(figsize=(40,60))

for i in range(20):
    
    plt.subplot(4,5,i+1)
    plt.plot(apple_cm_arr[i,:,0], apple_cm_arr[i,:,1], c="k", lw=0.5)
    plt.plot([min(apple_cm_arr[i,:,0])-0.1,min(apple_cm_arr[i,:,0])-0.1],
            [apple_cm_arr[i,0,1], apple_cm_arr[i,0,1]+1], c="k", lw=0.5) # cm scale
    plt.scatter(apple_cm_arr[i,:,0], apple_cm_arr[i,:,1], c="k", s=0.5)
    plt.scatter(apple_cm_arr[i,0,0], apple_cm_arr[i,0,1], s=4)
    plt.scatter(apple_cm_arr[i,res-1,0], apple_cm_arr[i,res-1,1], s=4)
    
    plt.title(mdata["apple_id"][i], fontsize=8)
    
    plt.gca().set_aspect("equal")
    plt.axis("off")
    
plt.subplots_adjust(hspace=0.4)
plt.suptitle(str(res*2-1) + " pseudo-landmarks")
plt.show()

# export figure
plt.savefig("C:/Users/User/Desktop/COOP02/code/apple_shape/output_final/Figure1d.pdf", )
```

<!-- The following sections of code are for analysis -->

### Analyzing Apple Dimensions

Using placed pseudo-landmarks representing leaves that are rotated upwards and scaled in centimeters from `apple_cm_arr`, calculate the following:

-   `width`: difference in centimeters between minimum and maximum x values in an oriented apple
-   `length`: difference in centimeters between minimum and maximum y values in an oriented apple
-   `area`: area of the apple in centimeters squared
-   `solidity`: the ratio of area to convex hull area
-   `asymmetry`: the Procrustes distance between the superimposed left and right sides of a apple outline. Lower values are more symmetric. Higher values are more asymmetric.

Data is stored in the `mdata` dataframe.

```{python}
#| label: Analyzing Apple Dimensions
# lists to store variables
width_list = []
length_list = []
area_list = []
solidity_list = []
# asymmetry_list = []

# for each apple . . .
for lf in range(len(apple_cm_arr)):
    
    # for calculating dimensions, we need non-scaled apple in centimeters
    curr_lf = apple_cm_arr[lf,:,:] # select current apple
    
    ############################
    ### CALCULATE DIMENSIONS ###
    ############################
    
    width = np.max(curr_lf[:,0])-np.min(curr_lf[:,0]) # calculate width
    length = np.max(curr_lf[:,1])-np.min(curr_lf[:,1]) # calculate length
    area = poly_area(curr_lf[:,0],curr_lf[:,1]) # calcualte area
    
    ##########################
    ### CALCULATE SOLIDITY ###
    ##########################
    
    hull = ConvexHull(curr_lf) # calculate convex hull of current apple
    vertices = hull.vertices # isolate vertex indices of convex hull
    convex_area = poly_area(curr_lf[vertices,0], curr_lf[vertices,1]) # calculate convex area
    solidity = area / convex_area # calculate solidity
    
    ##########################
    ### CALCULATE SYMMETRY ###
    ##########################
    
    # left_side = curr_lf[:(res-1)+1,] # isolate left side of apple
    # right_side = curr_lf[(res-1):,] # isolate right side of apple
    # right_side = right_side[::-1] # reverse the right side to align indices with left
    # 
    # # calculate procrustes distance between left and right side of apple
    # s1, s2, distance = procrustes(left_side, right_side) 
    
    # store data in lists
    width_list.append(width)
    length_list.append(length)
    area_list.append(area)
    solidity_list.append(solidity)
    # asymmetry_list.append(distance)
    
# add data to the mdata dataframe
mdata["width"] = width_list
mdata["length"] = length_list
mdata["area"] = area_list
mdata["solidity"] = solidity_list
# mdata["asymmetry"] = asymmetry_list

# save output so you don't have to run the code all the time
mdata.to_csv("C:/Users/User/Desktop/COOP02/code/apple_shape/output_final/20241219apple_metadata.csv", index = False)
```

#### REML Adjustment

```{r}
#| label: REML Input Data (Linear)

library(tidyverse)

# reading data
apple_traits <- read.csv("C:/Users/User/Desktop/COOP02/code/apple_shape/output_final/20241219apple_metadata.csv")

# averaging within the tree
apple_traits <- apple_traits %>% 
  group_by(nursery_id) %>% 
  mutate(across(all_of(c(8:11)), ~mean(.x, na.rm = TRUE), .names = "{col}"))

# save file
write.csv(apple_traits, file = "C:/Users/User/Desktop/COOP02/code/apple_shape/output_final/20241219apple_metadata_avg.csv")
```

<!-- Running the REML adjustment -->

```{r}
#| label: REML Adjustment Linear Measurements
library(lme4)
library(lsmeans)
library(lattice)
library(pbkrtest)
library(tidyverse)

# load in data
reml_design <- read.table("C:/Users/User/Desktop/COOP02/code/apple_shape/reml/20161024_ABC_design_grid_zm.txt", header = T)

apple_traits <- read.csv("C:/Users/User/Desktop/COOP02/code/apple_shape/output_final/20241219apple_metadata_avg.csv")

# Loading the metadata to match the design file
ph = apple_traits[match(reml_design[,"nursery_id"], apple_traits[,"nursery_id"]),] # Grab phenotype data in correct order
ph = cbind(reml_design, ph) #append phenotype data to reml tables
ph = ph[!is.na(ph[,"width"]),] # get rid of NAs
ph$apple_id = as.factor(as.character(ph$apple_id)) # make apple_id a factor

## check for each (will be slightly different)
length(unique(ph$nursery_id))
#743 nursery ids 
length(unique(ph$apple_id))
#534 unique cultivars (apple ids)

### WIDTH ---------
#Run the model
width_reml <- lmer(formula = width ~ apple_id + ( 1 | Block) + ( 1 | Block:rGrid) + ( 1 | Block:cGrid) + ( 1 | Block:cGrid:rGrid), data= ph,  REML = TRUE) #apple_id is fixed effect

#Get the least-square means
width_reml_lsmeans <- lsmeans(width_reml , "apple_id")
width_reml_lsmeans_predict <- predict(width_reml_lsmeans)
names(width_reml_lsmeans_predict) = levels(ph$apple_id)

# name before saving them (apple_id and trait), once at the end with them all appended
write.table(width_reml_lsmeans_predict, "all_reml_lsmeans_predict.txt", sep="\t", row.names=T, quote=F, col.names=T) 

predict <- read.delim("all_reml_lsmeans_predict.txt", col.names = "width_reml_lsmeans_predict")

#Compare the predicted ls_means by cultivar to a simple arithmetic mean by cultivar
mean_vals = aggregate(width ~ apple_id, data = ph, mean)
plot(mean_vals$width, width_reml_lsmeans_predict)
cor.test(mean_vals$width, width_reml_lsmeans_predict)
#cor width: 0.9958791


### LENGTH ---------
#Run the model
length_reml <- lmer(formula = length ~ apple_id + ( 1 | Block) + ( 1 | Block:rGrid) + ( 1 | Block:cGrid) + ( 1 | Block:cGrid:rGrid), data= ph,  REML = TRUE) #apple_id is fixed effect

#Get the least-square means
length_reml_lsmeans <- lsmeans(length_reml , "apple_id")
length_reml_lsmeans_predict <- predict(length_reml_lsmeans)
names(length_reml_lsmeans_predict) = levels(ph$apple_id)

# Adding to the data
predict <- cbind(predict, length_reml_lsmeans_predict)

#Compare the predicted ls_means by cultivar to a simple arithmetic mean by cultivar
mean_vals = aggregate(length ~ apple_id, data = ph, mean)
plot(mean_vals$length, length_reml_lsmeans_predict)
cor.test(mean_vals$length, length_reml_lsmeans_predict)
#cor length: 0.9989165

### AREA ---------
#Run the model
area_reml <- lmer(formula = area ~ apple_id + ( 1 | Block) + ( 1 | Block:rGrid) + ( 1 | Block:cGrid) + ( 1 | Block:cGrid:rGrid), data= ph,  REML = TRUE) #apple_id is fixed effect

#Get the least-square means
area_reml_lsmeans <- lsmeans(area_reml , "apple_id")
area_reml_lsmeans_predict <- predict(area_reml_lsmeans)
names(area_reml_lsmeans_predict) = levels(ph$apple_id)

# Adding to the data
predict <- cbind(predict, area_reml_lsmeans_predict)

#Compare the predicted ls_means by cultivar to a simple arithmetic mean by cultivar
mean_vals = aggregate(area ~ apple_id, data = ph, mean)
plot(mean_vals$area, area_reml_lsmeans_predict)
cor.test(mean_vals$area, area_reml_lsmeans_predict)
#cor area: 0.9981387 


### SOLIDITY ---------
#Run the model
solidity_reml <- lmer(formula = solidity ~ apple_id + ( 1 | Block) + ( 1 | Block:rGrid) + ( 1 | Block:cGrid) + ( 1 | Block:cGrid:rGrid), data= ph,  REML = TRUE) #apple_id is fixed effect

#Get the least-square means
solidity_reml_lsmeans <- lsmeans(solidity_reml , "apple_id")
solidity_reml_lsmeans_predict <- predict(solidity_reml_lsmeans)
names(solidity_reml_lsmeans_predict) = levels(ph$apple_id)

# Adding to the data
predict <- cbind(predict, solidity_reml_lsmeans_predict)

#Compare the predicted ls_means by cultivar to a simple arithmetic mean by cultivar
mean_vals = aggregate(solidity ~ apple_id, data = ph, mean)
plot(mean_vals$solidity, solidity_reml_lsmeans_predict)
cor.test(mean_vals$solidity, solidity_reml_lsmeans_predict)
#cor solidity: 0.9995955 


# ### ASYMMETRY ---------
# #Run the model
# asymmetry_reml <- lmer(formula = asymmetry ~ apple_id + ( 1 | Block) + ( 1 | Block:rGrid) + ( 1 | Block:cGrid) + ( 1 | Block:cGrid:rGrid), data= ph,  REML = TRUE) #apple_id is fixed effect
# 
# #Get the least-square means
# asymmetry_reml_lsmeans <- lsmeans(asymmetry_reml , "apple_id")
# asymmetry_reml_lsmeans_predict <- predict(asymmetry_reml_lsmeans)
# names(asymmetry_reml_lsmeans_predict) = levels(ph$apple_id)
# 
# # Adding to the data
# predict <- cbind(predict, asymmetry_reml_lsmeans_predict)
# 
# #Compare the predicted ls_means by cultivar to a simple arithmetic mean by cultivar
# mean_vals = aggregate(asymmetry ~ apple_id, data = ph, mean)
# plot(mean_vals$asymmetry, asymmetry_reml_lsmeans_predict)
# cor.test(mean_vals$asymmetry, asymmetry_reml_lsmeans_predict)
# #cor asymmetry: 0.9991252

predict <- tibble::rownames_to_column(predict, "apple_id")
write_delim(predict, file = "C:/Users/User/Desktop/COOP02/code/apple_shape/output_final/prediction_values.txt")
```

<!-- The following sections are the Procrustes Analysis -->

### Procrustes Analysis

Perform a Procrustes analysis to translate, scale, and rotate apple shapes

-   Select number of pseudo-landmarks and dimensions
-   Calculate the GPA mean apple shape using the `gpa_mean` function
-   Align all apples to the GPA mean
-   Store Procrustes super-imposed apples in an array, `proc_arr`
-   Calculate a PCA for all possible axes and their variance (the number of apples)
-   Calculate a PCA for just the axes needed for reconstruction of eigenleaves for morphospace (probably 2)

#### Calculate GPA Mean

```{python}
#| label: GPA Mean
landmark_num = (res*2)-1 # select number of landmarks
dim_num = 2 # select number of coordinate value dimensions

##########################
### CALCULATE GPA MEAN ###
##########################

mean_shape = gpa_mean(apple_cm_arr, landmark_num, dim_num)

################################
### ALIGN LEAVES TO GPA MEAN ###
################################

# array to store Procrustes aligned shapes
proc_arr = np.zeros(np.shape(apple_cm_arr)) 

for i in range(len(apple_cm_arr)):
  s1, s2, distance = procrustes(mean_shape, apple_cm_arr[i,:,:]) # calculate procrustes adjusted shape to ref for current apple
  proc_arr[i] = s2 # store procrustes adjusted shape to array
```

#### Calculate PC

```{python}
#| label: Calculate PC
#################################################
### FIRST, CALCULATE PERCENT VARIANCE ALL PCs ###
#################################################

######
PC_NUMBER = 78 # PC number = not number of apples, but features
#######

# use the reshape function to flatten to 2D
flat_arr = proc_arr.reshape(np.shape(proc_arr)[0], 
                                 np.shape(proc_arr)[1]*np.shape(proc_arr)[2]) 

pca_all = PCA(n_components=PC_NUMBER) 
PCs_all = pca_all.fit_transform(flat_arr) # fit a PCA for all data

# print out explained variance for each PC
print("PC: " + "var, " + "overall ") 
for i in range(len(pca_all.explained_variance_ratio_)):
    print("PC" + str(i+1) + ": " + str(round(pca_all.explained_variance_ratio_[i]*100,1)) + 
          "%, " + str(round(pca_all.explained_variance_ratio_.cumsum()[i]*100,1)) + "%"  )
```

```{python}
#| label: Calculate Number of PCs
#################################################
### NEXT, CALCULATE THE DESIRED NUMBER OF PCs ###
#################################################

new_mdata = pd.read_csv("C:/Users/User/Desktop/COOP02/code/apple_shape/output_final/20241219apple_metadata.csv")

##############
PC_NUMBER = 3
##############

pca = PCA(n_components=PC_NUMBER) 
PCs = pca.fit_transform(flat_arr) # fit a PCA for only desired PCs

# print out explained variance for each PC
print("PC: " + "var, " + "overall ") 
for i in range(len(pca.explained_variance_ratio_)):
    print("PC" + str(i+1) + ": " + str(round(pca.explained_variance_ratio_[i]*100,2)) + 
          "%, " + str(round(pca.explained_variance_ratio_.cumsum()[i]*100,2)) + "%"  )
    
# add PCs to dataframe for plotting
new_mdata["PC1"] = PCs[:,0]
new_mdata["PC2"] = PCs[:,1]
new_mdata["PC3"] = PCs[:,2]

# save output with PC
new_mdata.to_csv("C:/Users/User/Desktop/COOP02/code/apple_shape/output_final/20241219apple_metadata.csv", index = False)
```

#### REML Adjustment

```{r}
#| label: REML Input Data (PC)

# read in files
apple_traits <- read.csv("C:/Users/User/Desktop/COOP02/code/apple_shape/output_final/20241219apple_metadata.csv")

# averaging within the tree
apple_traits <- apple_traits %>% 
  group_by(nursery_id) %>% 
  mutate(across(all_of(c(12:14)), ~mean(.x, na.rm = TRUE), .names = "{col}"))

# save file
write.csv(apple_traits, file = "C:/Users/User/Desktop/COOP02/code/apple_shape/output_final/20241219apple_metadata_avg.csv")
```

<!-- Running the REML adjustment -->

```{r}
#| label: REML Adjustment PC
library(lme4)
library(lsmeans)
library(lattice)
library(pbkrtest)
library(tidyverse)

# read in data
reml_design <- read.table("C:/Users/User/Desktop/COOP02/code/apple_shape/reml/20161024_ABC_design_grid_zm.txt", header = T)

apple_traits <- read.csv("C:/Users/User/Desktop/COOP02/code/apple_shape/output_final/20241219apple_metadata_avg.csv")

# Loading the metadata to match the design file
ph = apple_traits[match(reml_design[,"nursery_id"], apple_traits[,"nursery_id"]),] # Grab phenotype data in correct order
ph = cbind(reml_design, ph) #append phenotype data to reml tables
ph = ph[!is.na(ph[,"PC1"]),] # get rid of NAs
ph$apple_id = as.factor(as.character(ph$apple_id)) # make apple_id a factor

## check for each (will be slightly different)
length(unique(ph$nursery_id))
#743 nursery ids 
length(unique(ph$apple_id))
#534 unique cultivars (apple ids)


### PC1 ---------
#Run the model
PC1_reml <- lmer(formula = PC1 ~ apple_id + ( 1 | Block) + ( 1 | Block:rGrid) + ( 1 | Block:cGrid) + ( 1 | Block:cGrid:rGrid), data= ph,  REML = TRUE) #apple_id is fixed effect

#Get the least-square means
PC1_reml_lsmeans <- lsmeans(PC1_reml , "apple_id")
PC1_reml_lsmeans_predict <- predict(PC1_reml_lsmeans)
names(PC1_reml_lsmeans_predict) = levels(ph$apple_id)

# Adding to the data
predict <- cbind(predict, PC1_reml_lsmeans_predict)

#Compare the predicted ls_means by cultivar to a simple arithmetic mean by cultivar
mean_vals = aggregate(PC1 ~ apple_id, data = ph, mean)
plot(mean_vals$PC1, PC1_reml_lsmeans_predict)
cor.test(mean_vals$PC1, PC1_reml_lsmeans_predict)
#cor PC1: 0.998253


### PC2 ---------
#Run the model
PC2_reml <- lmer(formula = PC2 ~ apple_id + ( 1 | Block) + ( 1 | Block:rGrid) + ( 1 | Block:cGrid) + ( 1 | Block:cGrid:rGrid), data= ph,  REML = TRUE) #apple_id is fixed effect

#Get the least-square means
PC2_reml_lsmeans <- lsmeans(PC2_reml , "apple_id")
PC2_reml_lsmeans_predict <- predict(PC2_reml_lsmeans)
names(PC2_reml_lsmeans_predict) = levels(ph$apple_id)

# Adding to the data
predict <- cbind(predict, PC2_reml_lsmeans_predict)

#Compare the predicted ls_means by cultivar to a simple arithmetic mean by cultivar
mean_vals = aggregate(PC2 ~ apple_id, data = ph, mean)
plot(mean_vals$PC2, PC2_reml_lsmeans_predict)
cor.test(mean_vals$PC2, PC2_reml_lsmeans_predict)
#cor PC2: 1


### PC3 ---------
#Run the model
PC3_reml <- lmer(formula = PC3 ~ apple_id + ( 1 | Block) + ( 1 | Block:rGrid) + ( 1 | Block:cGrid) + ( 1 | Block:cGrid:rGrid), data= ph,  REML = TRUE) #apple_id is fixed effect

#Get the least-square means
PC3_reml_lsmeans <- lsmeans(PC3_reml , "apple_id")
PC3_reml_lsmeans_predict <- predict(PC3_reml_lsmeans)
names(PC3_reml_lsmeans_predict) = levels(ph$apple_id)

# Adding to the data
predict <- cbind(predict, PC3_reml_lsmeans_predict)

#Compare the predicted ls_means by cultivar to a simple arithmetic mean by cultivar
mean_vals = aggregate(PC3 ~ apple_id, data = ph, mean)
plot(mean_vals$PC3, PC3_reml_lsmeans_predict)
cor.test(mean_vals$PC3, PC3_reml_lsmeans_predict)
#cor PC3: 1

# save data
write_delim(predict, file = "C:/Users/User/Desktop/COOP02/code/apple_shape/output_final/prediction_values.txt")
```

<!-- Supplemental Figure 1 -->

### Pairs Plot

```{r}
#| label: Pair Plot by Cultivar
library(tidyverse)
library(GGally)
library(readxl)
library(ggthemes)

# import data
predicted <- read.delim("C:/Users/User/Desktop/COOP02/code/apple_shape/output_final/prediction_values.txt", sep = "")
apple_data <- read.csv("C:/Users/User/Desktop/COOP02/code/apple_shape/output_final/20241219apple_metadata.csv") 
dataset <- read_excel("C:/Users/User/Desktop/COOP02/code/apple_shape/Dataset_S1.xlsx", sheet = "phenotype_data")

# combining datasets
all_data <- left_join(apple_data, predicted)
all_data <- all_data %>% left_join(dataset) %>% 
  dplyr::select(1:30, 39)

# converting data types
all_data$apple_id <- factor(all_data$apple_id)
all_data$weight_avg_16_harv <- as.numeric(all_data$weight_avg_16_harv)
all_data$release_year <- as.numeric(all_data$release_year)


# calculating aspect ratio
all_data <- all_data %>% mutate(aspect_ratio = width_reml_lsmeans_predict / length_reml_lsmeans_predict)

# relocating to plot easier
all_data <- all_data %>% relocate(aspect_ratio, .after = solidity_reml_lsmeans_predict)
all_data <- all_data %>% relocate(weight_avg_16_harv, .after = aspect_ratio)

# save data before with all apple id
write.csv(all_data, file = "C:/Users/User/Desktop/COOP02/code/apple_shape/output_final/20241219apple_metadata.csv")

# table how many binaries per tree
trees <- all_data %>% 
  group_by(nursery_id) %>% 
  summarise(total_count = n())

# save summary
write.csv(trees, file = "C:/Users/User/Desktop/COOP02/code/apple_shape/output_final/sample_size.csv", row.names = FALSE)

# filter for each unique genotype
all_data <- all_data %>% distinct(apple_id, .keep_all = TRUE)

# MAKE SURE THAT THIS IS PLOTTING CORRECTLY ADD IN BETTER LABELS
# plotting for 533 different apple_id with REML adjusted values
plot1 <- ggpairs(all_data, columns = c(16:24, 32), aes(fill = "grey", alpha = 0.5), columnLabels = c("Width", "Length", "Area", "Solidity", "Aspect Ratio", "Harvest Weight", "PC1", "PC2", "PC3", "Release Year"), upper = list(continuous = "points"), diag = list(continuous = wrap("densityDiag", fill = "black"))) + theme_few()

# save plot
ggsave("C:/Users/User/Desktop/COOP02/code/apple_shape/output_final/SupplementalFigure1_1219.pdf", plot = plot1, units = "in", width = 17, height = 15)
```

### Eigen Representation of PC

```{python}
#| label: Eigen Representation of PC
###################################################
### CREATE EIGEN REPRESENTATIONS OF FIRST 3 PCs ###
###################################################

new_mdata = pd.read_csv("C:/Users/User/Desktop/COOP02/code/apple_shape/output_final/20241219apple_metadata.csv")

# calculate standard deviations for each PC
PC1_std = new_mdata["PC1_reml_lsmeans_predict"].std()
PC2_std = new_mdata["PC2_reml_lsmeans_predict"].std()
PC3_std = new_mdata["PC3_reml_lsmeans_predict"].std()



# create list of lists of PC values to reconstruct

# std -4 -2 0 2 4
PC_vals = [[-6*PC1_std,0,0],
           [-4*PC1_std,0,0],
           [-2*PC1_std,0,0],
           [0*PC1_std,0,0],
           [2*PC1_std,0,0],
           [4*PC1_std,0,0],
           [6*PC1_std,0,0],
           [0,-6*PC2_std,0],
           [0,-4*PC2_std,0],
           [0,-2*PC2_std,0],
           [0,0*PC2_std,0],
           [0,2*PC2_std,0],
           [0,4*PC2_std,0],
           [0,6*PC2_std,0],
           [0,0,-6*PC3_std],
           [0,0,-4*PC3_std],
           [0,0,-2*PC3_std],
           [0,0,0*PC3_std],
           [0,0,2*PC3_std],
           [0,0,4*PC3_std],
           [0,0,6*PC3_std]]
           

plt.figure(figsize=(10,10))

counter = 1

for i in range(len(PC_vals)):
    
    # create inverse apple
    inv_leaf = pca.inverse_transform(np.array(PC_vals[i]))
    inv_x = inv_leaf[0::2] # select just inverse x vals
    inv_y = inv_leaf[1::2] # select just inverse y vals
    
    # plot inverse apple
    plt.subplot(3,7,counter)
    plt.fill(inv_x, inv_y, c="lightgray")
    plt.plot(inv_x, inv_y, c="gray")
    plt.gca().set_aspect("equal")
    plt.axis("off")
    
    
    counter += 1

plt.tight_layout()
plt.show()

# save figure
plt.savefig("C:/Users/User/Desktop/COOP02/code/apple_shape/output_final/SupplementalFigure2_1219.pdf")
```

### Morphospace (by Genotype)

Visualize a morphospace and classify apples by the factor of genotype

-   Plot a morphospace using the inverse transform of the PCA

#### Creating Morphospace

```{python}
#| label: Morphospace aspect_ratio (PC1 and PC3)
##########################
### CREATE MORPHOSPACE ###
##########################

averaged = pd.read_csv("C:/Users/User/Desktop/COOP02/code/apple_shape/output_final/20241219apple_metadata.csv")

# set plot parameters
plot_length = 40  # plot length in inches
plot_width = 20  # plot width in inches
numPC1 = 10  # set number of PC1 intervals
numPC3 = 9  # set number of PC2 intervals
hue = "aspect_ratio"  # select the factor to color by
s = 0.10  # set the scale of the eigen apples
lf_col = "lightgray"  # color of inverse eigen apples
lf_alpha = 0.5  # alpha of inverse eigen apple
pt_size = 20  # size of data points
pt_linewidth = 0  # lw of data points, set to 0 for no edges
pt_alpha = 1  # alpha of the data points
ax_label_fs = 12  # font size of the x and y axis titles
ax_tick_fs = 8  # font size of the axis ticks
face_col = "white"  # color of the plot background
grid_alpha = 0.5  # set the alpha of the grid
title = "Procrustean morphospace"  # set title

plt.figure(figsize=(plot_length, plot_width))

# note that PC2 is asymmetry and always = 0
PC1_vals = np.linspace(np.min(PCs[:, 0]), np.max(PCs[:, 0]), numPC1)  # create PC intervals
PC3_vals = np.linspace(np.min(PCs[:, 2]), np.max(PCs[:, 2]), numPC3)

for i in PC1_vals:  # for each PC1 interval
    for j in PC3_vals:  # for each PC2 interval

        pc1_val = i  # select the current PC1 val
        pc3_val = j  # select the current PC2 val

        # calculate the inverse eigenleaf
        inv_apple = pca.inverse_transform(np.array([pc1_val, 0, pc3_val]))
        inv_x = inv_apple[0::2]  # select just inverse x vals
        inv_y = inv_apple[1::2]  # select just inverse y vals

        # plot the inverse eigenleaf
        plt.fill(inv_x * s + pc1_val, inv_y * s + pc3_val, c=lf_col, alpha=lf_alpha)

# Normalize the color scale based on the aspect ratio limits
norm = plt.Normalize(vmin=0.86, vmax=1.28)  # normalization range for aspect ratio

# Create the scatter plot with the normalized colormap
scatter = sns.scatterplot(
    data=averaged,
    x="PC1_reml_lsmeans_predict",
    y="PC3_reml_lsmeans_predict",
    hue=hue,
    s=pt_size,
    linewidth=pt_linewidth,
    alpha=pt_alpha,
    palette="viridis",
    legend=False,  # Disable the default legend to avoid overlap with color bar
    hue_norm=norm  # Explicitly set normalization here
)

# Add a colorbar with correct normalization
sm = plt.cm.ScalarMappable(cmap="viridis", norm=norm)
sm.set_array([])  # ScalarMappable requires an empty array here
cbar = plt.colorbar(sm, ax=plt.gca())  # Add colorbar with the correct normalization
cbar.set_ticks(np.linspace(norm.vmin, norm.vmax, 7)) # adding the min and max values, plus evenly spaced out other values
cbar.set_label("Aspect Ratio")  # Label the color bar

# Set axis labels and titles
xlab = "PC1, " + str(round(pca.explained_variance_ratio_[0] * 100, 1)) + "%"
ylab = "PC3, " + str(round(pca.explained_variance_ratio_[2] * 100, 1)) + "%"
plt.xlabel(xlab, fontsize=ax_label_fs)
plt.ylabel(ylab, fontsize=ax_label_fs)
plt.xticks(fontsize=ax_tick_fs)
plt.yticks(fontsize=ax_tick_fs)
plt.gca().set_xlim([-0.09, 0.125])
plt.gca().set_ylim([-0.07, 0.08])
plt.gca().set_aspect("equal")
plt.gca().set_facecolor(face_col)
plt.grid(alpha=grid_alpha)
plt.gca().set_axisbelow(True)
#plt.title(title)

plt.show()

# save figure
plt.savefig("C:/Users/User/Desktop/COOP02/code/apple_shape/output_final/Figure3_1219.pdf")
```
